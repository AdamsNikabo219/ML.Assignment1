{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjH9yyeQHFO5"
      },
      "outputs": [],
      "source": [
        "from pickletools import optimize\n",
        "from re import A\n",
        "from turtle import forward\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import mnist\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    ガウス分布によるシンプルな初期化\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      ガウス分布の標準偏差\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "        Returns\n",
        "        ----------\n",
        "        W : ndarray shape with (n_nodes1, n_nodes2)\n",
        "          weights of hidden layer\n",
        "        \"\"\"\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        バイアスの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          後の層のノード数\n",
        "        Returns\n",
        "        ----------\n",
        "        B : ndarray shape with (n_nodes2, 1)\n",
        "          bias of hidden layer\n",
        "        \"\"\"\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B\n",
        "\n",
        "class SGD:\n",
        "    \"\"\"\n",
        "    Stochastic gradient descent\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : learning rate\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases for a layer\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : Instance of the layer before update\n",
        "        \"\"\"\n",
        "\n",
        "        #layer.w_x -= self.lr*layer.dw_x\n",
        "        #layer.b -= self.lr*layer.db\n",
        "        #layer.w_h -= self.lr*layer.dw_h\n",
        "\n",
        "        layer.w_x[...] = layer.w_x - self.lr * np.dot(layer.X.T, layer.dA) / len(layer.dA)\n",
        "        layer.b[...] = layer.b - self.lr * np.mean(layer.dA)\n",
        "        layer.w_h[...] = layer.w_h[...] - self.lr * np.dot(layer.h_t.T, layer.dA) / len(layer.dA)\n",
        "\n",
        "        return layer\n",
        "\n",
        "class tanh():\n",
        "  \"\"\"\n",
        "  Activation function - tangent\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "      pass\n",
        "  \n",
        "  def forward(self, A):\n",
        "    self.A = A\n",
        "    self.Z = np.tanh(self.A)\n",
        "\n",
        "    return self.Z\n",
        "\n",
        "  def backward(self, dZ):\n",
        "    ret = dZ * (1-self.Z**2)\n",
        "    return ret\n",
        "\n",
        "class sigmoid():\n",
        "  \"\"\"\n",
        "  Activation function - sigmoid\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "      pass\n",
        "  \n",
        "  def forward(self, A):\n",
        "    self.A = A\n",
        "    self.Z = 1/(1 + np.exp(-self.A))\n",
        "\n",
        "    return self.Z\n",
        "\n",
        "  def backward(self, dZ):\n",
        "    ret = dZ * (1-self.Z)*self.Z\n",
        "    return ret\n",
        "    \n",
        "class softmax():\n",
        "  \"\"\"\n",
        "  Activation function - softmax\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "      pass\n",
        "  \n",
        "  def forward(self, A):\n",
        "    #print(\"A:\", A)\n",
        "    #print(\"temp:\", np.exp(A - np.max(A)))\n",
        "    #print(\"forward temp:\", np.sum(np.exp(A-np.max(A))))\n",
        "    temp = np.exp(A - np.max(A))/np.sum(np.exp(A-np.max(A)), axis = 1, keepdims= True)\n",
        "    return temp\n",
        "\n",
        "  def backward(self, dZ):\n",
        "    return dZ\n",
        "  \n",
        "class ReLU():\n",
        "  \"\"\"\n",
        "  Activation function - relu\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "      pass\n",
        "  \n",
        "  def forward(self, A):\n",
        "    self.A = A\n",
        "    temp = np.maximum(self.A, 0)\n",
        "    return temp\n",
        "\n",
        "  def backward(self, dZ):\n",
        "    ret = np.where(self.A>0, dZ, 0)\n",
        "    return ret\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  stochastic gradient descent method\n",
        "  Parameters\n",
        "  -----------\n",
        "  lr : learning rate\n",
        "  \"\"\"\n",
        "  def __init__(self, lr):\n",
        "      self.lr = lr\n",
        "      self.hW = 0\n",
        "      self.hB = 0\n",
        "  \n",
        "  def update(self,layer):\n",
        "    \"\"\"\n",
        "    Updating the weights and biases of a layer\n",
        "    Parameters\n",
        "    -----------\n",
        "    layer : Instance of the layer before the update\n",
        "    \"\"\"\n",
        "    self.hW += layer.dW * layer.dW\n",
        "    self.hB = layer.dB * layer.dB\n",
        "\n",
        "    layer.W -= self.lr * layer.dW/(np.sqrt(self.hW) + 1e-7)\n",
        "    layer.B -=self.lr * layer.dB/(np.sqrt(self.hB) + 1e-7)\n",
        "\n",
        "    return layer\n",
        "\n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    Iterator to get a mini-batch\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : ndarray, shape (n_samples, n_features)\n",
        "      training data\n",
        "    y : ndarray, shape (n_samples, 1)\n",
        "      Label of training data\n",
        "    batch_size : int\n",
        "      batch size\n",
        "    seed : int\n",
        "      NumPy random seed\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1]        \n",
        "\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]\n",
        "\n",
        "##################### Problem 1 ###################\n",
        "# SimpleRNN forward propagation implementation\n",
        "\n",
        "class SimpleRNN:\n",
        "    \"\"\"\n",
        "    Simple recurrent neural network\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      Number of nodes in the previous layer\n",
        "    n_nodes2 : int\n",
        "      Number of nodes in the next layer\n",
        "    initializer : Instance of initialization method\n",
        "    optimizer : Instance of optimization method\n",
        "    activation : activation function \n",
        "    \"\"\"\n",
        "    def __init__(self, w_x, b, w_h, n_nodes1, n_nodes2, initializer, optimizer, activation):\n",
        "        self.optimizer = optimizer\n",
        "        self.initializer = initializer\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        self.activation = activation\n",
        "        # Initialization\n",
        "        # Initialize self.W and self.B using the #initialr method\n",
        "\n",
        "        self.WX = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.Wh = initializer.W(n_nodes2, n_nodes2)\n",
        "        self.B = initializer.B(1)\n",
        "\n",
        "        self.w_x = w_x\n",
        "        self.b = b\n",
        "        self.w_h = w_h\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        forward propagation\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : ndarray, shape (batch_size, n_nodes1)\n",
        "            input\n",
        "        Returns\n",
        "        ----------\n",
        "        h : ndarray, shape (batch_size, n_nodes2)\n",
        "            output\n",
        "        \"\"\"        \n",
        "        self.x_in = x\n",
        "        b_size, n_sequences, n_features = self.x_in.shape \n",
        "        h_t = np.zeros((b_size, self.n_nodes2))\n",
        "        A = np.empty((0, b_size, self.n_nodes2))\n",
        "        for i in range(n_sequences):\n",
        "            h_t = np.dot(self.x_in[:, i, :].reshape(b_size, n_features), self.w_x) + np.dot(h_t, self.w_h) + self.b\n",
        "            h_t = self.activation.forward(h_t)\n",
        "            A = np.vstack((A, h_t[np.newaxis,:])) #shape (sequence, batch, n_node)\n",
        "        A = A.transpose(1, 0, 2)\n",
        "        self.A = A\n",
        "        print(\"A:\", A)\n",
        "        return h_t\n",
        "        \n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        backward propagation\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : ndarray, shape (batch_size, n_nodes2)\n",
        "            Gradient descent from behind\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : ndarray, shape (batch_size, n_nodes1)\n",
        "            Gradient descent forward\n",
        "        \"\"\"\n",
        "        b_size, n_sequences, n_nodes = self.A.shape\n",
        "        dZ = np.zeros((n_sequences, b_size, self.n_nodes1))\n",
        "        for i in reversed(range(n_sequences)):\n",
        "            dA = self.activation.backward(dA)\n",
        "            dA = dA * (1 - self.A[:, i, :]**2) #shape (batch,n_nodes)\n",
        "            self.dA = dA\n",
        "            print(\"dA shape:\", self.dA.shape)\n",
        "            self.X = self.x_in[:, i, :]\n",
        "            self.h_t = self.A[:, i, :]\n",
        "            self = self.optimizer.update(self)\n",
        "            dA = np.dot(dA, self.w_h.T) #shape(batch, n_nodes)\n",
        "            dZ[i, :, :] = np.dot(dA, self.w_x.T) #shape (batch, n_features)\n",
        "            \n",
        "        dZ = dZ.transpose(1,0,2)\n",
        "        return dZ\n",
        "\n",
        "#################### Problem 2 ####################\n",
        "# Experiment of forward propagation with a small array\n",
        "\n",
        "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
        "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
        "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
        "batch_size = x.shape[0] # 1\n",
        "n_sequences = x.shape[1] # 3\n",
        "n_features = x.shape[2] # 2\n",
        "n_nodes = w_x.shape[1] # 4\n",
        "h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
        "b = np.array([1, 1, 1, 1]) # (n_nodes,)\n",
        "\n",
        "#w_x, b, w_h, n_nodes1, n_nodes2, initializer, optimizer, activation\n",
        "rnn = SimpleRNN(w_x, b, w_h, 2, 4, SimpleInitializer(0.01), SGD(lr=0.1), tanh())\n",
        "h = rnn.forward(x)\n",
        "print(\"h:\", h)\n"
      ]
    }
  ]
}