{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMw1bZyzvUKmmyc3fB87/mi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PeterYves/DIC-ML-Assignmnent/blob/main/Sprint_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`[Problem 1] Looking back on scratch`**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "zzWNrxFJ53G6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look back at the previous scratches so far, To implement deep learning we:\n",
        "\n",
        "- Had to initialize the weights\n",
        "- Needed an epoch loop\n",
        "- Coded the Activation Functions\n",
        "- Decided the learning rate, sizes, number of nodes and so on"
      ],
      "metadata": {
        "id": "pXx9nxg93hRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`[Problem 2] Consider the correspondence between scratch and TensorFlow`**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Q_GOsRT04m2T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cNKvZovewj7h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ef26932-5173-4345-b3f2-f082de34033b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 0.0000, val_loss : 0.1576, acc : 1.000, val_acc : 0.938\n",
            "Epoch 1, loss : 0.0000, val_loss : 1.6197, acc : 1.000, val_acc : 0.875\n",
            "Epoch 2, loss : 0.0000, val_loss : 1.6272, acc : 1.000, val_acc : 0.812\n",
            "Epoch 3, loss : 0.1918, val_loss : 6.6798, acc : 0.750, val_acc : 0.750\n",
            "Epoch 4, loss : 4.8505, val_loss : 18.5887, acc : 0.750, val_acc : 0.438\n",
            "Epoch 5, loss : 2.5552, val_loss : 8.9541, acc : 0.750, val_acc : 0.750\n",
            "Epoch 6, loss : 3.3191, val_loss : 16.3612, acc : 0.750, val_acc : 0.500\n",
            "Epoch 7, loss : 0.0000, val_loss : 2.3995, acc : 1.000, val_acc : 0.875\n",
            "Epoch 8, loss : 0.0000, val_loss : 0.3506, acc : 1.000, val_acc : 0.938\n",
            "Epoch 9, loss : 0.0000, val_loss : 0.0002, acc : 1.000, val_acc : 1.000\n",
            "test_acc : 0.900\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "dataset_path =\"Iris.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "\n",
        "y[y=='Iris-versicolor'] = 0\n",
        "y[y=='Iris-virginica'] = 1\n",
        "y = y.astype(int)[:, np.newaxis]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "class GetMiniBatch:\n",
        "\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
        "    return layer_output\n",
        "                               \n",
        "logits = example_net(X)\n",
        "\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above, we see that the Weights initialization is done through tf.Variable(tf.random_normal); Adam is used as an Optimizer and the activation function passes through tf.nn.relu."
      ],
      "metadata": {
        "id": "1kBPELtm5qLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`[Problem 3] Create a model of Iris using all three types of objective variables`**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "wR0dvh4M6PlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path =\"Iris.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "\n",
        "y[y=='Iris-setosa'] = 0\n",
        "y[y=='Iris-versicolor'] = 1\n",
        "y[y=='Iris-virginica'] = 2\n",
        "y = y.astype(int)[:, np.newaxis]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train)\n",
        "y_val_one_hot = enc.transform(y_val)\n",
        "y_test_one_hot = enc.transform(y_test)\n",
        "\n",
        "class GetMiniBatch:\n",
        "\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "    \n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 3\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
        "    return layer_output\n",
        "                             \n",
        "logits = example_net(X)\n",
        "\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKQRH9hW5q86",
        "outputId": "a24675a3-bbca-4c3b-d4ec-105e180ac57f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 0.9106, val_loss : 1.4981, acc : 0.833, val_acc : 0.917\n",
            "Epoch 1, loss : 4.0218, val_loss : 5.2426, acc : 0.667, val_acc : 0.750\n",
            "Epoch 2, loss : 0.0000, val_loss : 1.1544, acc : 1.000, val_acc : 0.833\n",
            "Epoch 3, loss : 0.0167, val_loss : 2.3645, acc : 1.000, val_acc : 0.917\n",
            "Epoch 4, loss : 0.0000, val_loss : 1.4888, acc : 1.000, val_acc : 0.917\n",
            "Epoch 5, loss : 0.0000, val_loss : 1.0480, acc : 1.000, val_acc : 0.958\n",
            "Epoch 6, loss : 0.0022, val_loss : 2.3852, acc : 1.000, val_acc : 0.917\n",
            "Epoch 7, loss : 0.0000, val_loss : 0.8214, acc : 1.000, val_acc : 0.958\n",
            "Epoch 8, loss : 0.0003, val_loss : 2.2164, acc : 1.000, val_acc : 0.917\n",
            "Epoch 9, loss : 0.0000, val_loss : 1.2561, acc : 1.000, val_acc : 0.917\n",
            "test_acc : 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`[Problem 4] Create a model of House Prices`**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lR2bDKIm8dMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "dataset_path =\"train.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "y = df[\"SalePrice\"]\n",
        "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "y = y.astype(int)[:, np.newaxis]\n",
        "y = np.log(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "class GetMiniBatch:\n",
        "\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
        "    return layer_output\n",
        "                               \n",
        "logits = example_net(X)\n",
        "loss_op =  tf.losses.mean_squared_error(labels=Y, predictions=logits)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "y_pred = logits\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    loss_list = []\n",
        "    val_loss_list = []\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "        loss = sess.run(loss_op, feed_dict={X: X_train, Y: y_train})\n",
        "        loss_list.append(loss)\n",
        "        val_loss = sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
        "        val_loss_list.append(val_loss)    \n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, loss, val_loss))\n",
        "    print(\"test_mse : {:.3f}\".format(loss))\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.plot(loss_list, label='loss')\n",
        "    plt.plot(val_loss_list, label='val_loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "kVltCfHA8cgA",
        "outputId": "69b340fe-af71-4de9-a154-f73c62be8e77"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 1431810.5000, val_loss : 1266386.0000\n",
            "Epoch 1, loss : 435381.0000, val_loss : 344322.1562\n",
            "Epoch 2, loss : 238564.0938, val_loss : 199283.9375\n",
            "Epoch 3, loss : 131337.4219, val_loss : 105711.9844\n",
            "Epoch 4, loss : 91054.5703, val_loss : 75644.3516\n",
            "Epoch 5, loss : 60767.7695, val_loss : 50849.3594\n",
            "Epoch 6, loss : 40744.7969, val_loss : 34555.8867\n",
            "Epoch 7, loss : 21191.4512, val_loss : 17736.6133\n",
            "Epoch 8, loss : 14139.3438, val_loss : 12142.8262\n",
            "Epoch 9, loss : 14811.5039, val_loss : 13140.6143\n",
            "test_mse : 14811.504\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`[Problem 5] Create a model of MNIST`**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "EecKPd-WEyTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "\n",
        "X_train = X_train.astype(np.float64)\n",
        "X_test = X_test.astype(np.float64)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "y_train = y_train.astype(int)[:, np.newaxis]\n",
        "y_test = y_test.astype(int)[:, np.newaxis]\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:])\n",
        "y_test_one_hot = enc.fit_transform(y_test[:])\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
        "\n",
        "class GetMiniBatch:\n",
        "\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 10\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
        "    return layer_output\n",
        "                               \n",
        "logits = example_net(X)\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "max_Y = (tf.argmax(Y, 1))\n",
        "max_Y_pred = tf.argmax(logits, 1)\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "    \n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynuAfEonE3Kh",
        "outputId": "6a71beb8-acde-4304-862b-e2289c2672b1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 0.9206, val_loss : 1.3047, acc : 0.600, val_acc : 0.596\n",
            "Epoch 1, loss : 0.8036, val_loss : 1.0003, acc : 0.600, val_acc : 0.663\n",
            "Epoch 2, loss : 0.5924, val_loss : 0.7427, acc : 0.800, val_acc : 0.742\n",
            "Epoch 3, loss : 0.2295, val_loss : 0.4555, acc : 0.900, val_acc : 0.871\n",
            "Epoch 4, loss : 0.0883, val_loss : 0.4319, acc : 1.000, val_acc : 0.879\n",
            "Epoch 5, loss : 0.2560, val_loss : 0.3976, acc : 0.900, val_acc : 0.888\n",
            "Epoch 6, loss : 0.0687, val_loss : 0.3918, acc : 1.000, val_acc : 0.897\n",
            "Epoch 7, loss : 0.0408, val_loss : 0.3755, acc : 1.000, val_acc : 0.901\n",
            "Epoch 8, loss : 0.0846, val_loss : 0.3619, acc : 1.000, val_acc : 0.906\n",
            "Epoch 9, loss : 0.0253, val_loss : 0.3923, acc : 1.000, val_acc : 0.903\n",
            "test_acc : 0.906\n"
          ]
        }
      ]
    }
  ]
}